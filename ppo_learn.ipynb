{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# gym\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#np.random.seed(0)\n",
    "#torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, hidden_size: int = 128, continuous_actions: bool = False):\n",
    "        super(Policy, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(n_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_outputs),\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(n_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "        \n",
    "        self.continuous_actions = continuous_actions\n",
    "        \n",
    "        if continuous_actions:\n",
    "            self.log_std = nn.Parameter(torch.zeros(n_outputs))\n",
    "        else:\n",
    "            self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        logits = self.actor(x) # distribution means if self.continuous_actions\n",
    "        \n",
    "        if self.continuous_actions:\n",
    "            std = torch.exp(self.log_std).expand_as(logits)\n",
    "            dist = Normal(logits, std)\n",
    "            \n",
    "        else:\n",
    "            probs = self.softmax(logits)\n",
    "            dist = Categorical(probs)\n",
    "        \n",
    "        return value, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Policy(\n",
    "    n_inputs=env.observation_space.shape[0],\n",
    "    n_outputs=env.action_space.n,\n",
    "    continuous_actions=False\n",
    ").to(device=device)\n",
    "\n",
    "optimiser = optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_mse = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.99\n",
    "EPSILON = 0.2\n",
    "CRITIC_DISCOUNT = 0.5\n",
    "ENTROPY_COEFF = 0.01\n",
    "BATCH_SIZE = 32\n",
    "BATCH_UPDATES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_step(model, observation) -> Dict[str, float | torch.Tensor | int | np.ndarray]:\n",
    "    value, dist = model.forward(torch.from_numpy(observation).to(device))\n",
    "    action = dist.sample()\n",
    "    new_observation, reward, terminated, truncated, _ = env.step(action.cpu().numpy())\n",
    "    mask = 1\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        new_observation, info = env.reset()\n",
    "        mask = 0\n",
    "    \n",
    "    return {\n",
    "        \"state\": torch.from_numpy(observation).to(device),\n",
    "        \"action\": action,\n",
    "        \"value\": value,\n",
    "        \"reward\": torch.FloatTensor([reward]).to(device),\n",
    "        \"log_prob\": dist.log_prob(action),\n",
    "        \"mask\": torch.FloatTensor([mask]).to(device),\n",
    "        \"new_state\": new_observation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, masks, discount_factor=0.99, gae_lambda=0.95):\n",
    "    values = values + [0] # last step won't have a next value\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + discount_factor * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + discount_factor * gae_lambda * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantage(returns, gae) -> torch.Tensor:\n",
    "    return normalize(gae - returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(model, observation) -> Dict[str, torch.Tensor]:\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_values = []\n",
    "    episode_rewards = []\n",
    "    episode_log_probs = []\n",
    "    episode_masks = []\n",
    "\n",
    "    while True:\n",
    "        step_data = play_step(model, observation)\n",
    "        observation = step_data[\"new_state\"]\n",
    "        episode_states.append(step_data[\"state\"])\n",
    "        episode_actions.append(step_data[\"action\"])\n",
    "        episode_values.append(step_data[\"value\"])\n",
    "        episode_rewards.append(step_data[\"reward\"])\n",
    "        episode_log_probs.append(step_data[\"log_prob\"])\n",
    "        episode_masks.append(step_data[\"mask\"])\n",
    "        if episode_masks[-1] == 0:\n",
    "            break\n",
    "\n",
    "    episode_gae = torch.tensor(compute_gae(episode_rewards, episode_values, episode_masks)).to(device)\n",
    "    \n",
    "    episode_values = torch.tensor(episode_values).to(device)\n",
    "    episode_log_probs = torch.tensor(episode_log_probs).to(device)\n",
    "    episode_states = torch.stack(episode_states).to(device)\n",
    "    episode_actions = torch.stack(episode_actions).to(device)\n",
    "    \n",
    "    return {\n",
    "        \"states\": episode_states,\n",
    "        \"actions\": episode_actions,\n",
    "        \"values\": episode_values,\n",
    "        \"log_probs\": episode_log_probs,\n",
    "        \"gae\": episode_gae\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episodes(model, batch_size, env) -> Dict[str, torch.Tensor]:\n",
    "    \n",
    "    states = torch.Tensor([]).to(device)\n",
    "    actions = torch.Tensor([]).to(device)\n",
    "    log_probs = torch.Tensor([]).to(device)\n",
    "    values = torch.Tensor([]).to(device)\n",
    "    gaes = torch.Tensor([]).to(device)\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        observation, _ = env.reset()\n",
    "        episode_data = play_episode(model, observation)\n",
    "        states = torch.cat([states, episode_data[\"states\"]])\n",
    "        actions = torch.cat([actions, episode_data[\"actions\"]])\n",
    "        log_probs = torch.cat([log_probs, episode_data[\"log_probs\"]])\n",
    "        values = torch.cat([values, episode_data[\"values\"]])\n",
    "        gaes = torch.cat([gaes, episode_data[\"gae\"]])\n",
    "    advantage = compute_advantage(gaes, values)\n",
    "    \n",
    "    return {\n",
    "        \"states\": states,\n",
    "        \"actions\": actions,\n",
    "        \"log_probs\": log_probs,\n",
    "        \"values\": values,\n",
    "        \"gaes\": gaes,\n",
    "        \"advantage\": advantage\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(data) -> Dict[str, float]:\n",
    "    total_loss = 0\n",
    "    total_actor_loss = 0\n",
    "    total_critic_loss = loss_mse(data[\"values\"], data[\"gaes\"]) * CRITIC_DISCOUNT\n",
    "    total_entropy_loss = 0\n",
    "    \n",
    "    loss_tracker = torch.tensor([])\n",
    "    for state, action, old_log_prob, values, gaes, advantage in zip(data[\"states\"], data[\"actions\"], data[\"log_probs\"], data[\"values\"], data[\"gaes\"], data[\"advantage\"]):\n",
    "        _, dist = model(state)\n",
    "        new_log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        ratio = (new_log_prob - old_log_prob).exp()\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1.0 - EPSILON, 1.0 + EPSILON) * advantage\n",
    "        actor_loss = -torch.min(surr1, surr2)\n",
    "        \n",
    "        #critic_loss = loss_mse(values, gaes)\n",
    "        \n",
    "        total_actor_loss += actor_loss\n",
    "        #total_critic_loss += critic_loss * CRITIC_DISCOUNT\n",
    "        total_entropy_loss += entropy * ENTROPY_COEFF\n",
    "        \n",
    "    total_loss = total_actor_loss + total_critic_loss - total_entropy_loss\n",
    "    loss_tracker = torch.cat([loss_tracker, total_loss.unsqueeze(0).detach().cpu()])\n",
    "        \n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimiser.step()\n",
    "        \n",
    "        \n",
    "    return {\"loss\": loss_tracker.mean().item()}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, env, batch_size, current_epoch, batch_updates) -> None:\n",
    "    total_loss = np.array([])\n",
    "    for batch_update in range(current_epoch, batch_updates + current_epoch):\n",
    "        data = sample_episodes(model, batch_size, env)\n",
    "        loss = fit_model(data)[\"loss\"]\n",
    "        total_loss = np.append(total_loss, loss)\n",
    "        if batch_update % 5 == 0:\n",
    "            print(f\"Epoch [{current_epoch} - {batch_update}] | Loss: {total_loss.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    train(model, env, BATCH_SIZE, epoch, BATCH_UPDATES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
