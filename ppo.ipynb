{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# gym\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, hidden_size: int = 128, continuous_actions: bool = False):\n",
    "        super(Policy, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(n_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_outputs),\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(n_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "        \n",
    "        self.continuous_actions = continuous_actions\n",
    "        \n",
    "        if continuous_actions:\n",
    "            self.log_std = nn.Parameter(torch.zeros(n_outputs))\n",
    "        else:\n",
    "            self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x).to(device)\n",
    "        \n",
    "        value = self.critic(x)\n",
    "        logits = self.actor(x) # distribution means if self.continuous_actions\n",
    "        \n",
    "        if self.continuous_actions:\n",
    "            std = torch.exp(self.log_std).expand_as(logits)\n",
    "            dist = Normal(logits, std)\n",
    "            \n",
    "        else:\n",
    "            probs = self.softmax(logits)\n",
    "            dist = Categorical(probs)\n",
    "        \n",
    "        return value, dist\n",
    "\n",
    "    def evaluate_state(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).to(device)\n",
    "        \n",
    "        return self.critic(state)\n",
    "    \n",
    "    def sample_action(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).to(device)\n",
    "        \n",
    "        logits = self.actor(state)\n",
    "        if self.continuous_actions:\n",
    "            std = torch.exp(self.log_std).expand_as(logits)\n",
    "            dist = Normal(logits, std)\n",
    "            \n",
    "        else:\n",
    "            probs = self.softmax(logits)\n",
    "            dist = Categorical(probs)\n",
    "            \n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action, log_prob\n",
    "    \n",
    "    def action_state_log_probs(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).to(device)\n",
    "            \n",
    "        if isinstance(action, np.ndarray):\n",
    "            action = torch.from_numpy(action).to(device)\n",
    "        \n",
    "        logits = self.actor(state)\n",
    "        if self.continuous_actions:\n",
    "            std = torch.exp(self.log_std).expand_as(logits)\n",
    "            dist = Normal(logits, std)\n",
    "            \n",
    "        else:\n",
    "            probs = self.softmax(logits)\n",
    "            dist = Categorical(probs)\n",
    "            \n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy().mean()\n",
    "        return log_prob, entropy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Policy(\n",
    "    n_inputs=env.observation_space.shape[0],\n",
    "    n_outputs=env.action_space.n,\n",
    "    continuous_actions=False\n",
    ").to(device=device)\n",
    "\n",
    "optimiser = optim.Adam(model.parameters(), lr=3e-4)\n",
    "loss_mse = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT_FACTOR = 0.99\n",
    "EPSILON = 0.2\n",
    "CRITIC_DISCOUNT = 0.5\n",
    "ENTROPY_COEFF = 0.01\n",
    "MAX_TRAJECTORY_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "TRAINING_STEPS = 200_000_000\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rtgs(batch_rewards):\n",
    "    \"\"\"\n",
    "        Compute the Reward-To-Go of each timestep in a batch given the rewards.\n",
    "\n",
    "        Parameters:\n",
    "            batch_rews - the rewards in a batch, Shape: (number of episodes, number of timesteps per episode)\n",
    "\n",
    "        Return:\n",
    "            batch_rtgs - the rewards to go, Shape: (number of timesteps in batch)\n",
    "    \"\"\"\n",
    "    # The rewards-to-go (rtg) per episode per batch to return.\n",
    "    # The shape will be (num timesteps per episode)\n",
    "    episode_rtgs = []\n",
    "\n",
    "    # Iterate through each episode\n",
    "    for trajectory in reversed(batch_rewards):\n",
    "        discounted_reward = 0 # The discounted reward so far\n",
    "        for reward in reversed(trajectory):\n",
    "            # Iterate through all rewards in the episode. We go backwards for smoother calculation of each\n",
    "            # discounted return (think about why it would be harder starting from the beginning)\n",
    "\n",
    "            discounted_reward = reward + discounted_reward * DISCOUNT_FACTOR\n",
    "            episode_rtgs.insert(0, discounted_reward)\n",
    "\n",
    "    # Convert the rewards-to-go into a tensor\n",
    "    episode_rtgs = torch.tensor(episode_rtgs, dtype=torch.float)\n",
    "\n",
    "    return episode_rtgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, masks, discount_factor=0.99, gae_lambda=0.95):\n",
    "    \n",
    "    \n",
    "    returns = []\n",
    "    for trajectory in reversed(range(BATCH_SIZE)):\n",
    "        gae = 0\n",
    "        values[trajectory].append(0) # last step doesn't exist\n",
    "    \n",
    "        for step in reversed(range(len(rewards[trajectory]))):\n",
    "            delta = rewards[trajectory][step] + discount_factor * values[trajectory][step + 1] * masks[trajectory][step] - values[trajectory][step]\n",
    "            gae = delta + discount_factor * gae_lambda * masks[trajectory][step] * gae\n",
    "            returns.insert(0, gae + values[trajectory][step])\n",
    "    returns = torch.tensor(returns, dtype=torch.float)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch() -> Dict[str, torch.Tensor]:\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    episode_lengths = []\n",
    "    values = []\n",
    "    masks = []\n",
    "    for _ in range(BATCH_SIZE):\n",
    "        trajectory_rewards = []\n",
    "        trajectory_masks = []\n",
    "        trajectory_values = []\n",
    "        observation, _ = env.reset()\n",
    "        for step in range(MAX_TRAJECTORY_SIZE):\n",
    "            observations.append(observation)\n",
    "            \n",
    "            action, log_prob = model.sample_action(observation)\n",
    "            value = model.evaluate_state(observation)\n",
    "            observation, reward, terminated, _, _ = env.step(action.cpu().numpy())\n",
    "            \n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            trajectory_rewards.append(reward)\n",
    "            trajectory_masks.append(1 - terminated)\n",
    "            trajectory_values.append(value)\n",
    "            \n",
    "            if terminated:\n",
    "                break\n",
    "           \n",
    "        episode_lengths.append(step + 1)\n",
    "        rewards.append(trajectory_rewards)\n",
    "        values.append(trajectory_values)\n",
    "        masks.append(trajectory_masks)\n",
    "    \n",
    "    observations = torch.tensor(np.array(observations))\n",
    "    actions = torch.tensor(actions)\n",
    "    log_probs = torch.tensor(log_probs)\n",
    "    #discounted_rewards = compute_rtgs(rewards)\n",
    "    gaes = calculate_gae(rewards, values, masks) # type: ignore\n",
    "    episode_lengths = torch.tensor(episode_lengths)\n",
    "    \n",
    "    return {\n",
    "        \"observations\": observations,\n",
    "        \"actions\": actions,\n",
    "        \"log_probs\": log_probs,\n",
    "        \"episode_lengths\": episode_lengths,\n",
    "        \"gaes\": gaes,\n",
    "        \"rewards\": torch.tensor(rewards[0]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gae(rewards, values, masks, discount_factor=0.99, gae_lambda=0.95):\n",
    "    batch_advantages = []\n",
    "    for ep_rews, ep_vals, ep_masks in zip(rewards, values, masks):\n",
    "        advantages = []\n",
    "        last_advantage = 0\n",
    "\n",
    "        for t in reversed(range(len(ep_rews))):\n",
    "            if t + 1 < len(ep_rews):\n",
    "                delta = ep_rews[t] + discount_factor * ep_vals[t+1] * (1 - ep_masks[t+1]) - ep_vals[t]\n",
    "            else:\n",
    "                delta = ep_rews[t] - ep_vals[t]\n",
    "\n",
    "            advantage = delta + discount_factor * gae_lambda * (1 - ep_masks[t]) * last_advantage\n",
    "            last_advantage = advantage\n",
    "            advantages.insert(0, advantage)\n",
    "\n",
    "        batch_advantages.extend(advantages)\n",
    "\n",
    "    return torch.tensor(batch_advantages, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    model.train()\n",
    "    total_timesteps = 0\n",
    "    while total_timesteps < TRAINING_STEPS:\n",
    "        batch = sample_batch()\n",
    "        observations = batch[\"observations\"].to(device)\n",
    "        \n",
    "        actions = batch[\"actions\"].to(device)\n",
    "        log_probs = batch[\"log_probs\"].to(device)\n",
    "        gaes = batch[\"gaes\"].to(device)\n",
    "        \n",
    "        episode_lengths = batch[\"episode_lengths\"]\n",
    "        total_timesteps += sum(episode_lengths).item()\n",
    "        rewards = batch[\"rewards\"]\n",
    "        \n",
    "        values = model.evaluate_state(observations).detach()\n",
    "        advantages = normalize(gaes - values)\n",
    "        losses = np.array([])\n",
    "        \n",
    "        for _ in range(EPOCHS):\n",
    "            values = model.evaluate_state(observations)\n",
    "            current_log_probs, entropy = model.action_state_log_probs(observations, actions)\n",
    "            \n",
    "            ratios = torch.exp(current_log_probs - log_probs)\n",
    "            \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - EPSILON, 1 + EPSILON) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            critic_loss = CRITIC_DISCOUNT * loss_mse(values, gaes.unsqueeze(1))\n",
    "            \n",
    "            \n",
    "            loss = actor_loss + critic_loss - ENTROPY_COEFF * entropy\n",
    "            losses = np.append(losses, loss.detach().cpu().numpy())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "        print(f\"Total timesteps: {total_timesteps} | Mean loss: {losses.mean()} | Mean rewards: {rewards.mean()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"ppo_2.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
